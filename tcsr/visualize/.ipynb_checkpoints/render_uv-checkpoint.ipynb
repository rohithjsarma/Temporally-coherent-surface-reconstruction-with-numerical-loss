{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project files.\n",
    "from tcsr.data.data_loader import DatasetClasses, DataLoaderDevicePairs\n",
    "from tcsr.train import helpers as tr_helpers\n",
    "from tcsr.visualize.renderer_pytorch3d import RendererPatchesUV\n",
    "import tcsr.visualize.helpers as vis_helpers\n",
    "from externals.jblib import helpers as jbh\n",
    "from externals.jblib import file_sys as jbfs\n",
    "from externals.jblib.deep_learning import torch_helpers as jbth\n",
    "\n",
    "# 3rd party.\n",
    "import torch\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import display\n",
    "from IPython.display import Image as iImage\n",
    "\n",
    "# Python std.\n",
    "import os\n",
    "import math\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-3-ad0e453c0546>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-ad0e453c0546>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    'our': '/cvlabdata2/home/jayakuma/testf_withsort/cat_walk/only_curv_neigh_9_alpha_1e-2__p10_bs4_DS_mode-neighbors5type-clean_centF_alignrotF$\u001b[0m\n\u001b[0m                                                                                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "### Settings.\n",
    "# Paths\n",
    "rel_paths_textures = {\n",
    "    'ortho': 'textures/ortho.png',\n",
    "    'diag': 'textures/diag.png'}\n",
    "file_rend_config = 'conf_patches.yaml'\n",
    "\n",
    "abs_path_out = ''  # <-- Set the abs. path for the output data.\n",
    "\n",
    "# Training runs paths.\n",
    "abs_paths_trruns = {  # <-- Fill in the abs. paths for the individual training runs.\n",
    "    'an': '',\n",
    "    'dsr': '',\n",
    "    'our': '/cvlabdata2/home/jayakuma/testf_withsort/cat_walk/only_curv_neigh_9_alpha_1e-2__p10_bs4_DS_mode-neighbors5type-clean_centF_alignrotF'\n",
    "}\n",
    "\n",
    "# Models.\n",
    "models_selected = ['our']  # <-- Choose the models to render the results for.\n",
    "\n",
    "# Data.\n",
    "subject = None  # <-- Only set as a string for DFAUST, CAPE, INRIA. E.g. '50002' for DFAUST.\n",
    "\n",
    "# Rendering settings - from `file_rend_config`.\n",
    "render_ds_specific = 'default'\n",
    "render_seq_specific = 'default_uv_camplane'\n",
    "texture_sel = 'diag'  # 'diag', 'orig'\n",
    "\n",
    "# Whether to render and save GIFs and/or PNGs.\n",
    "render_gif = True\n",
    "render_pngs = False\n",
    "\n",
    "# Meshes.\n",
    "mesh_edge_verts = 11\n",
    "num_patches = 10\n",
    "\n",
    "# Technical\n",
    "revert_cycle_gif = True\n",
    "img_size = 400\n",
    "bs = 16\n",
    "dev = torch.device('cuda')\n",
    "verbose = False\n",
    "\n",
    "# Process params.\n",
    "abs_path_base = os.path.abspath(os.path.curdir)\n",
    "abs_path_texture = jbfs.jn(abs_path_base, rel_paths_textures[texture_sel])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load data.\n",
    "conf_ds = jbh.load_conf(jbth.get_path_conf(abs_paths_trruns['our']))\n",
    "dataset = conf_ds['ds']\n",
    "sequence = conf_ds['sequences'][0]\n",
    "subject = None if (\n",
    "            (isinstance(subject, (list, tuple))\n",
    "             and len(subject) == 0) or subject is None) else [subject]\n",
    "ds = DatasetClasses[dataset](\n",
    "    num_pts=conf_ds['N'], subjects=subject, sequences=[sequence],\n",
    "    mode='within_seq', center=conf_ds['center'],\n",
    "    align_rot=conf_ds['align_rotation'], resample_pts=True,\n",
    "    with_reg=False, synth_rot=conf_ds['synth_rot'],\n",
    "    synth_rot_ang_per_frame=conf_ds['synth_rot_ang_per_frame'],\n",
    "    synth_rot_up=conf_ds['synth_rot_up'], noise=conf_ds['noise'],\n",
    "    pairing_mode=conf_ds.get('ds_pairing_mode', 'standard'),\n",
    "    pairing_mode_kwargs=conf_ds.get('ds_pairing_mode_params', None),\n",
    "    ds_type=conf_ds.get('ds_type', 'clean'))\n",
    "dl = DataLoaderDevicePairs(DataLoader(\n",
    "        ds, batch_size=bs, shuffle=False, num_workers=1,\n",
    "        drop_last=False), gpu=True)\n",
    "    \n",
    "# Create renderer.\n",
    "conf_rend_all = jbh.load_conf(jbfs.jn(abs_path_base, file_rend_config))\n",
    "confr = vis_helpers.get_rend_config(\n",
    "    conf_rend_all, dataset, ds_spec=render_ds_specific,\n",
    "    seq=sequence, seq_spec=render_seq_specific)\n",
    "renderer = RendererPatchesUV(\n",
    "    mesh_edge_verts, num_patches, abs_path_texture,\n",
    "    img_size=img_size, camera=confr['camera'], \n",
    "    light_loc=np.array(confr['light']['location']), \n",
    "    light_colors=confr['light']['colors'], \n",
    "    camera_animation=confr['camera_anim'], \n",
    "    uv_style=confr['texture']['style'], \n",
    "    uv_style_kwargs=None, gpu=True)\n",
    "\n",
    "# Process all models:\n",
    "images_all = {}\n",
    "process_models = list(abs_paths_trruns.keys()) \\\n",
    "    if models_selected is None else models_selected\n",
    "for mi, m in enumerate(process_models):\n",
    "    print(f\"Processing method {m}\")\n",
    "\n",
    "    # Load model.\n",
    "    path_trrun = abs_paths_trruns[m]\n",
    "    path_conf, path_trstate = jbth.get_path_conf_tr_state(path_trrun)\n",
    "    conf = jbh.load_conf(path_conf)\n",
    "    model = tr_helpers.create_model_train(conf)\n",
    "    model.load_state_dict(torch.load(path_trstate)['weights'])\n",
    "    _ = model.eval()\n",
    "    \n",
    "    # Prepare UVs for camera plane.\n",
    "    if confr['texture']['style'] == 'camera_plane':\n",
    "        pts = ds[confr['texture']['style_args']\n",
    "                 ['camera_plane']['ref_idx']]['pts'][:1]. \\\n",
    "            to(model.device)\n",
    "        vp = model.predict_mesh(\n",
    "            pts, mesh_edge_verts=mesh_edge_verts)[0]. \\\n",
    "            reshape((-1, 3)).detach().cpu().numpy()\n",
    "        renderer._prepare_uvs_cam_plane(\n",
    "            vp, cam_azi=confr['camera']['azi'], \n",
    "            cam_ele=confr['camera']['ele'])\n",
    "    \n",
    "    # Predict all samples.\n",
    "    images_all[m] = vis_helpers.render_uv_patches(\n",
    "        model, renderer, dl, confr, mesh_edge_verts, \n",
    "        model_type=conf['model'])\n",
    "    \n",
    "# Save visuals.\n",
    "num_imgs = len(ds)\n",
    "name_base = vis_helpers.name_from_config(\n",
    "    render_ds_specific, render_seq_specific)\n",
    "    \n",
    "# Create joined images.\n",
    "imgs_joined = []\n",
    "for i in range(num_imgs):\n",
    "    print(f\"\\rProcessing img {i + 1}/{num_imgs}\", end='')\n",
    "    imgs_joined.append(np.concatenate(\n",
    "        [images_all[m][i] for m in process_models], axis=1))\n",
    "imagesf_joined = np.stack(imgs_joined, axis=0)\n",
    "\n",
    "# Save joint pngs.\n",
    "if render_pngs:\n",
    "    print('Generating pngs.')\n",
    "    abs_path_pngs = jbfs.jn(abs_path_out, 'pngs', dataset, sequence,\n",
    "        name_base + f\"_size{img_size}_tex-{texture_sel}\")\n",
    "    jbfs.make_dir(abs_path_pngs)\n",
    "    for imi, im in enumerate(imagesf_joined):\n",
    "        print(f\"\\rSaving png {imi + 1}/{num_imgs}\", end='')\n",
    "        plt.imsave(jbfs.jn(abs_path_pngs, f\"fr_{imi:04d}.png\"), im)\n",
    "            \n",
    "# Render gif.\n",
    "if render_gif:\n",
    "    print('Generating gif.')\n",
    "    imagesi_joined = (imagesf_joined * 255.).astype(np.uint8)\n",
    "    fps = confr['gif']['fps']\n",
    "    subsample = confr['gif']['subsample']\n",
    "\n",
    "    abs_path_gif = jbfs.jn(abs_path_out, 'gif', dataset, sequence)\n",
    "    jbfs.make_dir(abs_path_gif)\n",
    "    nm_gif_out = f\"{name_base}_fps{fps}_ss{subsample}\" \\\n",
    "                 f\"_size{img_size}_tex-{texture_sel}.gif\"\n",
    "    pth_out = jbfs.jn(abs_path_gif, nm_gif_out)\n",
    "    \n",
    "    imgsgif = imagesi_joined[::subsample]\n",
    "    if revert_cycle_gif:\n",
    "        imgsgif = np.concatenate([imgsgif, imgsgif[-2:0:-1]], axis=0)\n",
    "    imageio.mimwrite(pth_out, imgsgif, fps=math.ceil(fps / subsample),\n",
    "                     palettesize=32, subrectangles=True)\n",
    "\n",
    "    # Display.\n",
    "    img_disp = iImage(filename=pth_out)\n",
    "    display(img_disp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
